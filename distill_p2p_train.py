import os
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from accelerate import Accelerator
from tqdm.auto import tqdm
from lpips import LPIPS
from pix2pix_turbo import Pix2Pix_Turbo, PatchDiscriminator
from distill_dataset import UnderwaterDistillationDataset

# --- Advanced Configuration ---
OUTPUT_DIR = "./checkpoints_distill_gan"
LEARNING_RATE_G = 5e-5
LEARNING_RATE_D = 1e-5 # Discriminator usually trains slower or with lower LR
BATCH_SIZE = 1
EPOCHS = 20

# Loss Weights
LAMBDA_L1 = 10.0      # Preserves content structure strongly
LAMBDA_LPIPS = 3.0    # Perceptual Similarity
LAMBDA_GAN = 0.5      # Adversarial Realism
LAMBDA_DISTILL = 2.0  # Teacher Distillation Strength

def main():
    # Initialize Accelerator
    accelerator = Accelerator(gradient_accumulation_steps=4)
    device = accelerator.device
    os.makedirs(OUTPUT_DIR, exist_ok=True)

    # 1. Initialize Models
    # Generator (Student)
    net_G = Pix2Pix_Turbo()
    net_G.set_train() # Enables LoRA and SkipConv gradients
    
    # Discriminator (Critic) - trained to distinguish Good Images from Generated Images
    net_D = PatchDiscriminator(input_nc=3, ndf=64).to(device)
    net_D.train()

    # 2. Loss Functions
    lpips_loss_fn = LPIPS(net='alex').to(device) # Perceptual loss
    criterion_GAN = nn.MSELoss() # LSGAN loss (more stable than BCE)
    criterion_L1 = nn.L1Loss()

    # 3. Dataset
    # Load teacher images if available
    # Use the augmented dataset yaml if it exists, otherwise fall back to labeled
    yaml_source = "dataset_augmented.yaml" if os.path.exists("dataset_augmented.yaml") else "dataset_labeled.yaml"
    print(f"Training with dataset: {yaml_source}")
    
    dataset = UnderwaterDistillationDataset(
        yaml_path=yaml_source, 
        root_dir="data_pipe/export_underwater2/dataset_20250921_093929", 
        teacher_dir="images_teacher",
        use_text_augmentation=True # Enable random text selection
    )
    train_dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    # 4. Optimizers
    # Optimizer for Generator (optimizes LoRA + Skip Convs)
    optimizer_G = torch.optim.AdamW(filter(lambda p: p.requires_grad, net_G.parameters()), lr=LEARNING_RATE_G)
    
    # Optimizer for Discriminator (optimizes full discriminator)
    optimizer_D = torch.optim.AdamW(net_D.parameters(), lr=LEARNING_RATE_D)

    # 5. Prepare Distributed Training
    net_G, net_D, optimizer_G, optimizer_D, train_dataloader = accelerator.prepare(
        net_G, net_D, optimizer_G, optimizer_D, train_dataloader
    )

    # 6. Training Loop
    global_step = 0
    for epoch in range(EPOCHS):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch+1}/{EPOCHS}")
        
        for step, batch in enumerate(train_dataloader):
            # Unpack data
            bad_img = batch["bad_image"].to(device)   # Input (degraded)
            good_img = batch["good_image"].to(device) # Ground Truth (real)
            prompt = batch["enhancement"]             # Instruction
            
            # Optional: Teacher Image (Pre-generated by Qwen)
            teacher_img = batch.get("teacher_image")
            if teacher_img is not None:
                teacher_img = teacher_img.to(device)

            # ---------------------
            #  Train Discriminator
            # ---------------------
            optimizer_D.zero_grad()
            
            # Generate fake image (detach to avoid updating G here)
            with torch.no_grad():
                fake_img = net_G(bad_img, prompt[0] if isinstance(prompt, list) else prompt)
            
            # Real loss
            pred_real = net_D(good_img)
            loss_D_real = criterion_GAN(pred_real, torch.ones_like(pred_real))
            
            # Fake loss
            pred_fake = net_D(fake_img.detach())
            loss_D_fake = criterion_GAN(pred_fake, torch.zeros_like(pred_fake))
            
            # Total Discriminator Loss
            loss_D = (loss_D_real + loss_D_fake) * 0.5
            accelerator.backward(loss_D)
            optimizer_D.step()

            # -----------------
            #  Train Generator
            # -----------------
            optimizer_G.zero_grad()
            
            # Re-generate fake image (with gradients tracking this time)
            fake_img_for_g = net_G(bad_img, prompt[0] if isinstance(prompt, list) else prompt)
            
            # 1. GAN Loss (Fool the discriminator)
            pred_fake_g = net_D(fake_img_for_g)
            loss_gan_g = criterion_GAN(pred_fake_g, torch.ones_like(pred_fake_g))
            
            # 2. Key Supervision: Pixel Loss (Structure)
            loss_pixel = criterion_L1(fake_img_for_g, good_img)
            
            # 3. Key Supervision: Perceptual Loss (Texture/Details)
            loss_lpips = lpips_loss_fn(fake_img_for_g, good_img).mean()
            
            # 4. Teacher Distillation Loss (Optional)
            loss_distill = torch.tensor(0.0, device=device)
            if teacher_img is not None:
                # Force student to mimic teacher's high-level features/style
                loss_distill = lpips_loss_fn(fake_img_for_g, teacher_img).mean()
            
            # Total Generator Loss
            loss_G = (loss_gan_g * LAMBDA_GAN) + \
                     (loss_pixel * LAMBDA_L1) + \
                     (loss_lpips * LAMBDA_LPIPS) + \
                     (loss_distill * LAMBDA_DISTILL)
            
            accelerator.backward(loss_G)
            optimizer_G.step()

            # Logging
            if accelerator.sync_gradients:
                progress_bar.update(1)
                global_step += 1
                progress_bar.set_postfix({
                    "L_G": loss_G.item(), 
                    "L_D": loss_D.item(),
                    "Distill": loss_distill.item()
                })

        # Save Checkpoint
        if accelerator.is_main_process:
            unwrapped_model = accelerator.unwrap_model(net_G)
            unwrapped_model.save_model(os.path.join(OUTPUT_DIR, f"pix2pix_turbo_gan_epoch_{epoch+1}.pt"))
            # Optional: Save discriminator if you plan to resume training
            # torch.save(accelerator.unwrap_model(net_D).state_dict(), os.path.join(OUTPUT_DIR, f"discriminator_epoch_{epoch+1}.pt"))

if __name__ == "__main__":
    main()
